- We decided to build the program in python in order to take advantage of third party libraries
- To keep everything oncise in a single file we forked n processes to mimic the virtual machines
- Tried to use fork to loop through and generate new processes however we kept running into errors with correctly exiting the program/random.randint was generating the same integer for every process. We could use the os.urandom function but that required converting from a random byte sequence to an integer between 1 and 6 and it was slightly ugly
-Using the multiprocessing library was much easier and cleaner, the code was simplified and we were able to use functions like randint 
-Currently the program loops through and generates n processes as specified by the -n flag, and for each new process generates the random int between 1 and 6
-In order to setup multiple logs using the logging library, we pulled some code from stackoverflow. It seems like a better idea to have multiple logs then try to have them all write to the same log. If we did the latter, we could get weird race conditions and the logs may not actually represent the state of the system. 
-Other than pulling the code from stackoverflow, the separate logs were relatiely simple to implement
-Having trouble printing to indivdual process log files, setup generates the log files and no error is thrown when I try to log to the file but nothing is added. The general log still works after creating new logs
-....silly error, named the logger incorrectly, thats what I get for not saving the name of the logger into a variable, now onto setting up the sockets and network queue
-A design decision that may come up is whether to setup the listening sockets as each process is created, or have it wait until the processes are running before it sets up the connections. Im guessing we will probably have to use the latter 
-Or maybe just one general listening? idk still looking into it, trying to use http://www.binarytides.com/python-socket-server-code-example/ as an example
-  